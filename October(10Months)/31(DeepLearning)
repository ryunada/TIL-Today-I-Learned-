# 텐서플로우 튜토리얼에 나오는 패션 아이템을 DNN으로 분류하는 코드
https://www.tensorflow.org/datasets/catalog/fashion_mnist


```python
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import datasets, layers, models

fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

train_images = train_images / 255.0 
test_images = test_images / 255.0

model = models.Sequential() 
model.add(layers.Flatten(input_shape=(28, 28))) 
model.add(layers.Dense(128, activation='relu')) 
model.add(layers.Dense(10, activation='softmax'))
model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])

model.fit(train_images, train_labels, epochs = 5)
```

    /opt/anaconda3/envs/tensorflow2/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3
      warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
    2022-11-01 09:48:44.062018: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2
    To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.


    Epoch 1/5
    1875/1875 [==============================] - 2s 807us/step - loss: 0.4965 - accuracy: 0.8253
    Epoch 2/5
    1875/1875 [==============================] - 2s 809us/step - loss: 0.3734 - accuracy: 0.8654
    Epoch 3/5
    1875/1875 [==============================] - 2s 838us/step - loss: 0.3374 - accuracy: 0.8795
    Epoch 4/5
    1875/1875 [==============================] - 2s 866us/step - loss: 0.3126 - accuracy: 0.8846
    Epoch 5/5
    1875/1875 [==============================] - 2s 801us/step - loss: 0.2933 - accuracy: 0.8926





    <keras.callbacks.History at 0x7fcaaca80130>



# CNN을 이용한 영상 분류


```python
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import datasets, layers, models
fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

train_images = train_images.reshape((60000, 28, 28, 1)) 
test_images = test_images.reshape((10000, 28, 28, 1))

train_images = train_images / 255.0 
test_images = test_images / 255.0

model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) 
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu')) 
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten()) 
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=5)
test_loss, test_acc = model.evaluate(test_images, test_labels)
print('정확도:', test_acc)
```

    Epoch 1/5
    1875/1875 [==============================] - 16s 8ms/step - loss: 0.4995 - accuracy: 0.8190
    Epoch 2/5
    1875/1875 [==============================] - 16s 9ms/step - loss: 0.3177 - accuracy: 0.8843
    Epoch 3/5
    1875/1875 [==============================] - 16s 9ms/step - loss: 0.2728 - accuracy: 0.9002
    Epoch 4/5
    1875/1875 [==============================] - 18s 10ms/step - loss: 0.2430 - accuracy: 0.9095
    Epoch 5/5
    1875/1875 [==============================] - 18s 9ms/step - loss: 0.2187 - accuracy: 0.9180
    313/313 [==============================] - 1s 4ms/step - loss: 0.2721 - accuracy: 0.9021
    정확도: 0.9021000266075134


# DNN도 CNN처럼 모델 만들어서 DNN,CNN성능 테스트


```python
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import datasets, layers, models

fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

train_images = train_images / 255.0 
test_images = test_images / 255.0

model = models.Sequential() 
model.add(layers.Flatten(input_shape=(28, 28)))
model.add(layers.Dense(64, activation='relu')) 
model.add(layers.Dense(30, activation='relu'))
model.add(layers.Dense(64, activation='relu')) 
model.add(layers.Dense(34, activation='relu')) 
model.add(layers.Dense(10, activation='softmax'))
model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])

model.fit(train_images, train_labels, epochs = 5)
```

    Epoch 1/5
    1875/1875 [==============================] - 2s 838us/step - loss: 0.5492 - accuracy: 0.8038
    Epoch 2/5
    1875/1875 [==============================] - 2s 826us/step - loss: 0.3911 - accuracy: 0.8570
    Epoch 3/5
    1875/1875 [==============================] - 2s 862us/step - loss: 0.3536 - accuracy: 0.8692
    Epoch 4/5
    1875/1875 [==============================] - 2s 870us/step - loss: 0.3320 - accuracy: 0.8771
    Epoch 5/5
    1875/1875 [==============================] - 2s 821us/step - loss: 0.3146 - accuracy: 0.8834





    <keras.callbacks.History at 0x7fcabb148160>



결론 -> 아무리 DNN을 느리거나 줄여도 CNN 보다 성능이 좋아지지는 않는다.
---

---
# 생성적 적대 신경망(GAN; Generative Adversarial Network)


```python
#-*- coding: utf-8 -*-

from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout
from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, UpSampling2D, Conv2D
from tensorflow.keras.models import Sequential, Model

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt

#이미지가 저장될 폴더가 없다면 만듭니다.
import os
if not os.path.exists("./gan_images"):
    os.makedirs("./gan_images")

np.random.seed(3)
tf.random.set_seed(3)

#생성자 모델을 만듭니다.
generator = Sequential()
generator.add(Dense(128*7*7, input_dim=100, activation=LeakyReLU(0.2))) 
# 128 : 임의로 정한 노드수, 7*7 : 이미지의 최초 크기, input_dim = 100 : 100차원 크기의 랜덤 벡터를 준비해 집어넣으라는 뜻
generator.add(BatchNormalization())
generator.add(Reshape((7, 7, 128)))
generator.add(UpSampling2D())
generator.add(Conv2D(64, kernel_size=5, padding='same'))
generator.add(BatchNormalization())
generator.add(Activation(LeakyReLU(0.2)))
generator.add(UpSampling2D())
generator.add(Conv2D(1, kernel_size=5, padding='same', activation='tanh'))

#판별자 모델을 만듭니다.
discriminator = Sequential()
discriminator.add(Conv2D(64, kernel_size=5, strides=2, input_shape=(28, 28, 1), padding='same'))
discriminator.add(Activation(LeakyReLU(0.2)))
discriminator.add(Dropout(0.3))
discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding='same'))
discriminator.add(Activation(LeakyReLU(0.2)))
discriminator.add(Dropout(0.3))
discriminator.add(Flatten())
discriminator.add(Dense(1, activation='sigmoid'))
discriminator.compile(loss='binary_crossentropy', optimizer='adam')
discriminator.trainable = False

#생성자와 판별자 모델을 연결시키는 gan 모델을 만듭니다.
ginput = Input(shape=(100,))
dis_output = discriminator(generator(ginput))
gan = Model(ginput, dis_output)
gan.compile(loss='binary_crossentropy', optimizer='adam')
gan.summary()

#신경망을 실행시키는 함수를 만듭니다.
def gan_train(epoch, batch_size, saving_interval):

  # MNIST 데이터 불러오기

  (X_train, _), (_, _) = mnist.load_data()  # 앞서 불러온 적 있는 MNIST를 다시 이용합니다. 단, 테스트과정은 필요없고 이미지만 사용할 것이기 때문에 X_train만 불러왔습니다.
  X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')
  X_train = (X_train - 127.5) / 127.5  # 픽셀값은 0에서 255사이의 값입니다. 이전에 255로 나누어 줄때는 이를 0~1사이의 값으로 바꾸었던 것인데, 여기서는 127.5를 빼준 뒤 127.5로 나누어 줌으로 인해 -1에서 1사이의 값으로 바뀌게 됩니다.
  #X_train.shape, Y_train.shape, X_test.shape, Y_test.shape

  true = np.ones((batch_size, 1))
  fake = np.zeros((batch_size, 1))

  for i in range(epoch):
          # 실제 데이터를 판별자에 입력하는 부분입니다.
          idx = np.random.randint(0, X_train.shape[0], batch_size)
          imgs = X_train[idx]
          d_loss_real = discriminator.train_on_batch(imgs, true)

          #가상 이미지를 판별자에 입력하는 부분입니다.
          noise = np.random.normal(0, 1, (batch_size, 100))
          gen_imgs = generator.predict(noise)
          d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)

          #판별자와 생성자의 오차를 계산합니다.
          d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
          g_loss = gan.train_on_batch(noise, true)

          print('epoch:%d' % i, ' d_loss:%.4f' % d_loss, ' g_loss:%.4f' % g_loss)

        # 이부분은 중간 과정을 이미지로 저장해 주는 부분입니다. 본 장의 주요 내용과 관련이 없어
        # 소스코드만 첨부합니다. 만들어진 이미지들은 gan_images 폴더에 저장됩니다.
          if i % saving_interval == 0:
              #r, c = 5, 5
              noise = np.random.normal(0, 1, (25, 100))
              gen_imgs = generator.predict(noise)

              # Rescale images 0 - 1
              gen_imgs = 0.5 * gen_imgs + 0.5

              fig, axs = plt.subplots(5, 5)
              count = 0
              for j in range(5):
                  for k in range(5):
                      axs[j, k].imshow(gen_imgs[count, :, :, 0], cmap='gray')
                      axs[j, k].axis('off')
                      count += 1
              fig.savefig("gan_images/gan_mnist_%d.png" % i)

gan_train(4001, 32, 200)  #4000번 반복되고(+1을 해 주는 것에 주의), 배치 사이즈는 32,  200번 마다 결과가 저장되게 하였습니다.
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_1 (InputLayer)        [(None, 100)]             0         
                                                                     
     sequential_3 (Sequential)   (None, 28, 28, 1)         865281    
                                                                     
     sequential_4 (Sequential)   (None, 1)                 212865    
                                                                     
    =================================================================
    Total params: 1,078,146
    Trainable params: 852,609
    Non-trainable params: 225,537
    _________________________________________________________________
    1/1 [==============================] - 0s 91ms/step
    epoch:0  d_loss:0.7053  g_loss:0.6912
    1/1 [==============================] - 0s 66ms/step
    1/1 [==============================] - 0s 34ms/step
    epoch:1  d_loss:0.4624  g_loss:0.3281
    1/1 [==============================] - 0s 36ms/step
    epoch:2  d_loss:0.5688  g_loss:0.1024
    1/1 [==============================] - 0s 39ms/step
    epoch:3  d_loss:0.6580  g_loss:0.0836
    1/1 [==============================] - 0s 43ms/step
    ...
    epoch:3998  d_loss:0.5214  g_loss:1.8311
    1/1 [==============================] - 0s 32ms/step
    epoch:3999  d_loss:0.5663  g_loss:1.7691
    1/1 [==============================] - 0s 33ms/step
    epoch:4000  d_loss:0.4196  g_loss:2.0103
    1/1 [==============================] - 0s 30ms/step


    /var/folders/hf/9cldw65x7j71qr4hjbw44yjc0000gn/T/ipykernel_27630/2334526313.py:94: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
      fig, axs = plt.subplots(5, 5)



    



    
![png](output_8_3.png)
    



    
![png](output_8_4.png)
    



    
![png](output_8_5.png)
    



    
![png](output_8_6.png)
    



    
![png](output_8_7.png)
    



    
![png](output_8_8.png)
    



    
![png](output_8_9.png)
    



    
![png](output_8_10.png)
    



    
![png](output_8_11.png)
    



    
![png](output_8_12.png)
    



    
![png](output_8_13.png)
    



    
![png](output_8_14.png)
    



    
![png](output_8_15.png)
    



    
![png](output_8_16.png)
    



    
![png](output_8_17.png)
    



    
![png](output_8_18.png)
    



    
![png](output_8_19.png)
    



    
![png](output_8_20.png)
    



    
![png](output_8_21.png)
    



    
![png](output_8_22.png)
    


# 오토인코더(Auto-Encoder, AE)


```python
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape
import matplotlib.pyplot as plt
import numpy as np

#MNIST데이터 셋을 불러옵니다.

(X_train, _), (X_test, _) = mnist.load_data()
```


```python
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape
import matplotlib.pyplot as plt
import numpy as np

#MNIST데이터 셋을 불러옵니다.

(X_train, _), (X_test, _) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255

#생성자 모델을 만듭니다.
autoencoder = Sequential()

# 인코딩 부분입니다.
autoencoder.add(Conv2D(16, kernel_size = 3, padding = 'same', input_shape = (28,28,1), activation = 'relu'))
autoencoder.add(MaxPooling2D(pool_size = 2, padding = 'same'))
autoencoder.add(Conv2D(8, kernel_size = 3, activation = 'relu', padding = 'same'))
autoencoder.add(MaxPooling2D(pool_size = 2, padding = 'same'))
autoencoder.add(Conv2D(8, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu'))

# 디코딩 부분이 이어집니다. 
autoencoder.add(Conv2D(8, kernel_size = 3, padding = 'same', activation = 'relu'))
autoencoder.add(UpSampling2D())
autoencoder.add(Conv2D(8, kernel_size = 3, padding = 'same', activation = 'relu'))
autoencoder.add(UpSampling2D())
autoencoder.add(Conv2D(16, kernel_size = 3, activation = 'relu'))
autoencoder.add(UpSampling2D())
autoencoder.add(Conv2D(1, kernel_size = 3, padding = 'same',activation = 'sigmoid'))

# 전체 구조를 확인해 봅니다.
autoencoder.summary()

# 컴파일 및 학습을 하는 부분입니다.
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.fit(X_train, X_train, epochs=50, batch_size=128, validation_data=(X_test, X_test))

#학습된 결과를 출력하는 부분입니다.
random_test = np.random.randint(X_test.shape[0], size=5)  #테스트할 이미지를 랜덤하게 불러옵니다.
ae_imgs = autoencoder.predict(X_test)  #앞서 만든 오토인코더 모델에 집어 넣습니다.

plt.figure(figsize=(7, 2))  #출력될 이미지의 크기를 정합니다.

for i, image_idx in enumerate(random_test):    #랜덤하게 뽑은 이미지를 차례로 나열합니다.
   ax = plt.subplot(2, 7, i + 1) 
   plt.imshow(X_test[image_idx].reshape(28, 28))  #테스트할 이미지를 먼저 그대로 보여줍니다.
   ax.axis('off')
   ax = plt.subplot(2, 7, 7 + i +1)
   plt.imshow(ae_imgs[image_idx].reshape(28, 28))  #오토인코딩 결과를 다음열에 출력합니다.
   ax.axis('off')
plt.show()

```

    Model: "sequential_5"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d_7 (Conv2D)           (None, 28, 28, 16)        160       
                                                                     
     max_pooling2d_2 (MaxPooling  (None, 14, 14, 16)       0         
     2D)                                                             
                                                                     
     conv2d_8 (Conv2D)           (None, 14, 14, 8)         1160      
                                                                     
     max_pooling2d_3 (MaxPooling  (None, 7, 7, 8)          0         
     2D)                                                             
                                                                     
     conv2d_9 (Conv2D)           (None, 4, 4, 8)           584       
                                                                     
     conv2d_10 (Conv2D)          (None, 4, 4, 8)           584       
                                                                     
     up_sampling2d_2 (UpSampling  (None, 8, 8, 8)          0         
     2D)                                                             
                                                                     
     conv2d_11 (Conv2D)          (None, 8, 8, 8)           584       
                                                                     
     up_sampling2d_3 (UpSampling  (None, 16, 16, 8)        0         
     2D)                                                             
                                                                     
     conv2d_12 (Conv2D)          (None, 14, 14, 16)        1168      
                                                                     
     up_sampling2d_4 (UpSampling  (None, 28, 28, 16)       0         
     2D)                                                             
                                                                     
     conv2d_13 (Conv2D)          (None, 28, 28, 1)         145       
                                                                     
    =================================================================
    Total params: 4,385
    Trainable params: 4,385
    Non-trainable params: 0
    _________________________________________________________________
    Epoch 1/50
    469/469 [==============================] - 13s 27ms/step - loss: 0.2156 - val_loss: 0.1405
    Epoch 2/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.1265 - val_loss: 0.1162
    Epoch 3/50
    469/469 [==============================] - 927s 2s/step - loss: 0.1120 - val_loss: 0.1072
    Epoch 4/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.1058 - val_loss: 0.1024
    Epoch 5/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.1021 - val_loss: 0.0997
    Epoch 6/50
    469/469 [==============================] - 1024s 2s/step - loss: 0.0996 - val_loss: 0.0978
    Epoch 7/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0977 - val_loss: 0.0960
    Epoch 8/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0962 - val_loss: 0.0944
    Epoch 9/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0949 - val_loss: 0.0934
    Epoch 10/50
    469/469 [==============================] - 13s 29ms/step - loss: 0.0939 - val_loss: 0.0925
    Epoch 11/50
    469/469 [==============================] - 1004s 2s/step - loss: 0.0931 - val_loss: 0.0915
    Epoch 12/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0923 - val_loss: 0.0909
    Epoch 13/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0917 - val_loss: 0.0904
    Epoch 14/50
    469/469 [==============================] - 13s 27ms/step - loss: 0.0911 - val_loss: 0.0897
    Epoch 15/50
    469/469 [==============================] - 942s 2s/step - loss: 0.0905 - val_loss: 0.0893
    Epoch 16/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0900 - val_loss: 0.0888
    Epoch 17/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0895 - val_loss: 0.0883
    Epoch 18/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0891 - val_loss: 0.0878
    Epoch 19/50
    469/469 [==============================] - 988s 2s/step - loss: 0.0887 - val_loss: 0.0876
    Epoch 20/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0884 - val_loss: 0.0871
    Epoch 21/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0881 - val_loss: 0.0868
    Epoch 22/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0877 - val_loss: 0.0866
    Epoch 23/50
    469/469 [==============================] - 970s 2s/step - loss: 0.0875 - val_loss: 0.0865
    Epoch 24/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0872 - val_loss: 0.0862
    Epoch 25/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0870 - val_loss: 0.0861
    Epoch 26/50
    469/469 [==============================] - 13s 28ms/step - loss: 0.0867 - val_loss: 0.0855
    Epoch 27/50
    469/469 [==============================] - 588s 1s/step - loss: 0.0865 - val_loss: 0.0852
    Epoch 28/50
    469/469 [==============================] - 14s 30ms/step - loss: 0.0863 - val_loss: 0.0853
    Epoch 29/50
    469/469 [==============================] - 15s 31ms/step - loss: 0.0861 - val_loss: 0.0850
    Epoch 30/50
    469/469 [==============================] - 14s 31ms/step - loss: 0.0859 - val_loss: 0.0850
    Epoch 31/50
    469/469 [==============================] - 16s 33ms/step - loss: 0.0857 - val_loss: 0.0849
    Epoch 32/50
    469/469 [==============================] - 15s 32ms/step - loss: 0.0855 - val_loss: 0.0843
    Epoch 33/50
    469/469 [==============================] - 15s 33ms/step - loss: 0.0854 - val_loss: 0.0841
    Epoch 34/50
    469/469 [==============================] - 16s 35ms/step - loss: 0.0851 - val_loss: 0.0840
    Epoch 35/50
    469/469 [==============================] - 16s 33ms/step - loss: 0.0849 - val_loss: 0.0838
    Epoch 36/50
    469/469 [==============================] - 16s 33ms/step - loss: 0.0848 - val_loss: 0.0836
    Epoch 37/50
    469/469 [==============================] - 16s 34ms/step - loss: 0.0846 - val_loss: 0.0839
    Epoch 38/50
    469/469 [==============================] - 16s 34ms/step - loss: 0.0844 - val_loss: 0.0831
    Epoch 39/50
    469/469 [==============================] - 16s 34ms/step - loss: 0.0842 - val_loss: 0.0831
    Epoch 40/50
    469/469 [==============================] - 16s 35ms/step - loss: 0.0840 - val_loss: 0.0830
    Epoch 41/50
    469/469 [==============================] - 20s 43ms/step - loss: 0.0838 - val_loss: 0.0826
    Epoch 42/50
    469/469 [==============================] - 19s 40ms/step - loss: 0.0837 - val_loss: 0.0825
    Epoch 43/50
    469/469 [==============================] - 19s 40ms/step - loss: 0.0835 - val_loss: 0.0827
    Epoch 44/50
    469/469 [==============================] - 19s 40ms/step - loss: 0.0834 - val_loss: 0.0822
    Epoch 45/50
    469/469 [==============================] - 20s 42ms/step - loss: 0.0832 - val_loss: 0.0823
    Epoch 46/50
    469/469 [==============================] - 19s 41ms/step - loss: 0.0830 - val_loss: 0.0821
    Epoch 47/50
    469/469 [==============================] - 19s 41ms/step - loss: 0.0829 - val_loss: 0.0819
    Epoch 48/50
    469/469 [==============================] - 19s 41ms/step - loss: 0.0828 - val_loss: 0.0817
    Epoch 49/50
    469/469 [==============================] - 20s 42ms/step - loss: 0.0826 - val_loss: 0.0815
    Epoch 50/50
    469/469 [==============================] - 20s 43ms/step - loss: 0.0825 - val_loss: 0.0814
    313/313 [==============================] - 1s 4ms/step



    

